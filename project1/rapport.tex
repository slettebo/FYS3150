\documentclass[norsk,a4paper,12pt]{article}
\usepackage[T1]{fontenc} %for å bruke æøå
\usepackage[utf8]{inputenc}
\usepackage{graphicx} %for å inkludere grafikk
\usepackage{verbatim} %for å inkludere filer med tegn LaTeX ikke liker
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{amsmath}




\bibliographystyle{plain}

\title{FYS3150 - Report 1: Vector- and matrix operations in C++ (using Armadillo)}
\author{Magnus Holm Slettebø}
\date{\today}
\begin{document}

\maketitle

\begin{abstract}
The purpose of this project has been to familiarize ourselves with C++ programming and various vector- and matrix operations in Armadillo, through solving a linear, second-order differential equation. We've developed a specialized algorithm for solving a tridiagonal matrix equation and compared this (with regards to execution time and number of floating point operations) with the built-in LU-factorization and solve functions of Armadillo. Concluding that if we know that we're dealing with a tridiagonal positive definite matrix we will save valuable computation time by using the specialized algorithm.
\end{abstract}


\section{Introduction}
Our task is to write a program that solves a linear, second-order differential equation. We are going rewrite the equation as a matrix equation and exploit the fact that we're dealing with a tridiagonal positive definite matrix. Using a specialized algorithm we will reduce the number of floating point operations and thus the execution time of the program. We will compare the number of required floating point operations with standard Gaussian elimination and LU-factorization to comment on the efficiency of the algorithm. We will also compare the execution time of our algorithm and with the execution time of Armadillo's LU-decomposition and solver, for matrices of the size ($n \times n$ for $n \in [10,100,1000,10000,100000]$). We will also compute the relative error for the various $n$'s to investigate numerical precision. All source code files can be found at \url{https://github.com/slettebo/FYS3150/tree/master/project1}.


\section{Theory}
Taken from the assignment text:

Many important differential equations in the Sciences can be written as 
linear second-order differential equations 
\[
\frac{d^2y}{dx^2}+k^2(x)y = f(x),
\]
where $f$ is normally called the inhomogeneous term and $k^2$ is a real function.

A classical equation from electromagnetism is Poisson's equation.
The electrostatic potential $\Phi$ is generated by a localized charge
distribution $\rho ({\bf r})$.   In three dimensions 
it reads
\[
\nabla^2 \Phi = -4\pi \rho ({\bf r}).
\]
With a spherically symmetric $\Phi$ and $\rho ({\bf r})$  the equations
simplifies to a one-dimensional equation in $r$, namely
\[
\frac{1}{r^2}\frac{d}{dr}\left(r^2\frac{d\Phi}{dr}\right) = -4\pi \rho(r),
\]
which can be rewritten via a substitution $\Phi(r)= \phi(r)/r$ as
\[
\frac{d^2\phi}{dr^2}= -4\pi r\rho(r).
\]
The inhomogeneous term $f$ or source term is given by the charge distribution
$\rho$  multiplied by $r$ and the constant $-4\pi$.

We will rewrite this equation by letting $\phi\rightarrow u$ and 
$r\rightarrow x$. 
The general one-dimensional Poisson equation reads then 
\[
-u''(x) = f(x).
\]




\section{The problem}
In this project we will solve the one-dimensional Poissson equation
with Dirichlet boundary conditions by rewriting it as a set of linear equations.

To be more explicit we will solve the equation
\[
-u''(x) = f(x), \hspace{0.5cm} x\in(0,1), \hspace{0.5cm} u(0) = u(1) = 0.
\]
and we define the discretized approximation  to $u$ as $v_i$  with 
grid points $x_i=ih$   in the interval from $x_0=0$ to $x_{n+1}=1$.
The step length or spacing is defined as $h=1/(n+1)$. 
We have then the boundary conditions $v_0 = v_{n+1} = 0$.
We  approximate the second
derivative of $u$ with 
\[
   -\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i  \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,\dots,n,
\]
where $f_i=f(x_i)$.

\subsection*{a) Rewriting the equation in matrix-form:}

This can be written as a linear set of equations of the form:

\[
(-1) v_{i-1} + (2) v_i + (-1) v_{i+1} = h^2f_i = d_i \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,\dots,n.
\]
(Note: I've named the solution matrix-elements $d_i$ in stead of $\tilde{b_i}$, because this made my program easier to read).

which, when $v_0$ = $v_{n+1}$ = 0, can be written as:

\begin{equation}
\begin{split}
0 + (2) v_{1} + (-1) v_{2} + = d_1 \\
(-1) v_{1} + (2) v_{2} + (-1) v_{3} = d_2 \\
\dots \\
(-1) v_{n-2} + (2) v_{n-1} + (-1) v_{n} = d_{n-1} \\
(-1) v_{n-1} + (2) v_{n} + 0 = d_{n} \\
\end{split}
\end{equation}
	
This is obviously equivalent to writing:
\begin{equation}
    				\left(\begin{array}{cccccc}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{array} \right)\left(\begin{array}{c}
                           v_1\\
                           v_2\\
                           \dots \\
                           \dots \\
                           v_{n-1} \\
                           v_{n}\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           d_1\\
                           d_2\\
                           \dots \\
                           \dots \\
                           d_{n-1} \\
                           d_n\\
                      \end{array} \right).	
\end{equation}


\subsection*{b) The algorithm}

\begin{equation}
    {\bf Av} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_n & b_n \\
                      \end{array} \right)\left(\begin{array}{c}
                           v_1\\
                           v_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           v_n\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           d_1\\
                           d_2\\
                           \dots \\
                           \dots \\
                           \dots \\
                           d_n\\
                      \end{array} \right) = {\bf d}.
\end{equation}


\subsubsection*{Forward step:}
Using row reductions we calculate the new elements in $\bf A$ and $\bf d$.
Below is the algorithm from my program:
\verbatiminput{fwdalgo.txt}

\begin{itemize}
\item Computing new $a_i$'s is redundant as all will be zero and will not be used again. We save 3 FLOPS by omitting this operation.
\item Multiplying the 'constant' with $c_i$ is redundant because all $c_i$'s are -1, so we save 1 FLOP by realizing that this is the same as adding the 'constant'.
\end{itemize}

So in the forward step we're performing $4(n-1)$ FLOPS.


\subsubsection*{Backward step:}
Starting from the bottom we calculate $v_n$, then we insert this in the 'equation' above and solve for $v_{n-1}$.

The algorithm from my program:
\verbatiminput{bckwalgo.txt}


\begin{itemize}
\item Multiplying the $c_i$ with  the $v_{i+1}$ is redundant because all $c_i$'s are -1, so we save 1 FLOP by realizing that this is the same as adding $v_{i+1}$.
\end{itemize}
So in the backward step we're performing $2(n-1)$ FLOPS.


\subsubsection*{Floating point operations}
\begin{itemize}
\item In total the 'tridiagonal positive definite'-specialized algorithm requires $6(n-1)$ FLOPS.
\item Standard Gaussian elimination takes approximately $\frac{2n^3}{3}$ FLOPS (source: wikipedia.org).
\item Computing an LU factorization takes approximately $\frac{2n^3}{3}$ FLOPS and solving $Ly = b$ and $Ux = y$ requires approximately $2n^2$ FLOPS, because any $n x n$ triangular system can be solved in about $n^2$ FLOPS (source: "Linear algebra and it's applications" by David Lay).
\end{itemize}

Thus we conclude that this specialized algorithm is much faster for large $n$'s compared with the the generalized algorithms when solving tridiagonal matrix problems.


\section{My program}
Below is the code of my program "main.cpp": 

\small
\verbatiminput{main.cpp}
\normalsize


\section{Solution}
Using my program I've solved the problem for $n \times n$-sized matrices for $n \in [10,100,1000]$. Below are plots of the numerical solutions (together with the analytical solution).

\subsection*{$\bf{n=10}$}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{n10.png}     
    \caption{Plot of numerical- and analytical solutions for $n = 10$}
    \label{fig:plot_n10}
\end{figure}

Figure \ref{fig:plot_n10} shows the plot of the numerical and analytical solution of the differential equation for n=10. We can clearly see that we've got some deviation from the analytical expression, but this is to be expected with few grid points, and thus a large step size ($h$ = $\frac{1}{11}$), which makes our approximation of the second derivative a very rough approximation.



\subsection*{$\bf{n=100}$}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{n100.png}     
    \caption{Plot of numerical- and analytical solutions for $n = 100$}
    \label{fig:plot_n100}
\end{figure}

Figure \ref{fig:plot_n100} shows the plot of the numerical and analytical solution of the differential equation for n=100. The numerical and analytical solutions overlap, from looking at the plot alone it's hard to tell how accurate this solution is: but they seem to be in agreement. The increased number of grid points give smaller step size ($h$ = $\frac{1}{101}$) and thus a better approximation of the second derivative.

\subsection*{$\bf{n=1000}$}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{n1000.png}     
    \caption{Plot of numerical- and analytical solutions for $n = 1000$}
    \label{fig:plot_n1000}
\end{figure}

Figure \ref{fig:plot_n1000} shows the plot of the numerical and analytical solution of the differential equation for n=1000. The numerical and analytical solutions overlap, as with the case of $n=100$ it's hard to see any differences by looking at the plot alone: they seem to be in perfect agreement.
 
\section{c) Relative error vs. step length}
To get a better indication of how close the numerical and analytical solutions are for the various step lengths I've plotted the (10-logarithm of the) relative error vs the (10-logarithm of the) step length. I've also included the (approximate) numbers in a table.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{rel_error.png}     
    \caption{Plot of relative error vs. step length}
    \label{fig:rel_error}
\end{figure}


\begin{table}[ht!]
	\caption{$log_{10}(\epsilon_i)$ vs. $log_{10}(h_i)$.}
    \begin{tabular}{|l|l|l|}
    \hline
    $n$     & $log_{10}(h(n))$			  & $log_{10} (\epsilon (n))$ \\ \hline
    $10^1$ & $10^{-2}$                & -1.18                            \\ \hline
    $10^2$ & $10^{-3}$                & -3.09                            \\ \hline
    $10^3$ & $10^{-4}$                & -5.08                            \\ \hline
    $10^4$ & $10^{-5}$                & -7.08                            \\ \hline
    $10^5$ & $10^{-6}$                & -8.84                            \\ \hline
    $10^6$ & $10^{-7}$                & -6.08                            \\ \hline
    \end{tabular}
    \label{tab:rel_error}
\end{table}

In figure \ref{fig:rel_error} I've plotted the $log_{10}(\epsilon_i)$ vs. $log_{10}(h_i)$. I did this for $n \in [10^1, 10^2,10^3,10^4,10^5,10^6]$ to get a rough estimate of where we get loss in numerical precision. The values are also listed in table \ref{tab:rel_error}. From the plot we see that the relative error decreases until $h = \frac{1}{10^6+1}$. At that point the step length is so small that we get loss of numerical precision (round off errors).



\section{d) Execution time: my tridiagonal solver vs. Armadillo's LU/solver}

\begin{table}[ht!]
	\caption{Execution time: my specialized triagonal solver vs. Armadillo's generalized LU-decomposition- and solver-functions}
    \begin{tabular}{|l|l|l|}
    \hline
    $n$     & Tridiagonal solver (s)  & Armadillo (s)  \\ \hline
    $10^1$ & $1 \cdot 10^{-6}$                & $4.2 \cdot 10^{-5}$      \\ \hline
    $10^2$ & $9 \cdot 10^{-6}$                & $4.35 \cdot 10^{-4}$      \\ \hline
    $10^3$ & $3.6 \cdot 10^{-5}$              & $1.99 \cdot 10^{-2}$      \\ \hline
    \end{tabular}
    \label{tab:exec_time}
\end{table}

Table \ref{tab:exec_time} shows the execution time of the algorithm in my program and Armadillo, run on my laptop. We clearly see that the specialized tridiagonal solver is superior to Armadillo's generalized solvers when it comes to execution time. 

\subsubsection*{Limits of standard LU decomposition using Armadillo}

LU-decomposition of a $10^5$ x $10^5$ matrix with Armadillo yields the following output in the terminal:
\verbatiminput{armadillo_100000_output.txt}

When I followed Armadillo's suggestion to enable ARMA\_64BIT\_WORD i got the following output in the terminal:
\verbatiminput{arma_64_output.txt}

My laptops 8 GBs of RAM is apparently insufficient for using Armadillo's LU-decomposition on a $10^5$ x $10^5$ matrix.


\section{Conclusion}
When dealing with tridiagonal positive definite matrices we clearly see that the specialized algorithm is superior to the generalized solver of Armadillo:
\begin{itemize}
\item The number of floating point operations is dramatically reduced: $6(n-1)$ (tridiagonal) vs. $\frac{2n^3}{3}$ (LU-decomposition) + $2n^2$ (solver).
\item The execution time is much faster (see table \ref{tab:exec_time})
\item More memory efficient: on my laptop I could solve this for $n=10^6$, while using Armadillo i ran out of memory with $n=10^5$
\end{itemize}

So in conclusion we clearly see that writing/using specialized programs can make a huge difference when dealing with enormous data sets.



\end{document}
